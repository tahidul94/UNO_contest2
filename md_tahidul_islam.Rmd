---
title: ""
author: "Islam MD Tahidul"
date: ""
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
  word_document: default
---
```{r}
setwd("/Users/mdtahidulislam/Desktop/machine_learning_stat/contest2")
```

##############################
Best model start from line 294 
##############################
 
 
#Data Loading and Initial Exploration
```{r}
train_data <- read.csv("contest2data/train.csv")
test_data <- read.csv("contest2data/test.csv")

#summary(train_data)
#str(train_data)
#head(train_data)
```


#data exploration
```{r}
library(hexbin)

ggplot(train_data, aes(x = duration, y = total)) +
  geom_hex(bins = 30) +  # Adjust bins for different resolution
  labs(title = "Hexbin Plot of Duration vs Total",
       x = "Duration",
       y = "Total") +
  theme_minimal()
```


```{r}
# 1. Bar Plot: Credit Levels
ggplot(train_data, aes(x = credit)) +
  geom_bar(fill = "steelblue") +
  ggtitle("Bar Plot of Credit Levels")


# 2. Box Plot: Total by Credit
ggplot(train_data, aes(x = credit, y = total, fill = credit)) +
  geom_boxplot() +
  ggtitle("Box Plot of Total by Credit Level")


# 3. Histogram: Duration Distribution
ggplot(train_data, aes(x = duration)) +
  geom_histogram(bins = 30, fill = "darkgreen") +
  ggtitle("Histogram of Duration")


# 4. Bar Plot: Fraud Counts
ggplot(train_data, aes(x = as.factor(fraud))) +
  geom_bar(fill = "tomato") +
  ggtitle("Bar Plot of Fraud Counts")


# 5. Box Plot: Scans by Fraud Status
ggplot(train_data, aes(x = as.factor(fraud), y = scans, fill = as.factor(fraud))) +
  geom_boxplot() +
  ggtitle("Box Plot of Scans by Fraud Status")


```

```{r}
# Calculating mean, median, and standard deviation for numerical features
summary_stats <- data.frame(
  Feature = c("Duration", "Total", "Scans", "VoidedScans", "AttemptsWoScan", "ModifiedQuantities"),
  Mean = sapply(train_data[, c("duration", "total", "scans", "voidedScans", "attemptsWoScan", "modifiedQuantities")], mean),
  Median = sapply(train_data[, c("duration", "total", "scans", "voidedScans", "attemptsWoScan", "modifiedQuantities")], median),
  SD = sapply(train_data[, c("duration", "total", "scans", "voidedScans", "attemptsWoScan", "modifiedQuantities")], sd)
)
print(summary_stats)

```

```{r}
# Compute correlation matrix
numeric_data <- train_data[, c("duration", "total", "scans", "voidedScans", "attemptsWoScan", "modifiedQuantities")]
cor_matrix <- cor(numeric_data)

# Plotting the heatmap
library(ggplot2)
library(reshape2)
melted_cor <- melt(cor_matrix)
ggplot(melted_cor, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  ggtitle("Heat Map of Correlation Coefficients") +
  theme_minimal()

```


#Data Preprocessing
```{r}
sum(is.na(train_data))
train_data$credit <- as.factor(train_data$credit)
train_data$fraud <- as.factor(train_data$fraud)
# Adjusting factor levels
train_data$fraud <- factor(train_data$fraud, levels = c(0, 1), labels = c("NonFraud", "Fraud"))

```


#Feature Engineering
```{r}
# Example1: Interaction between duration and total amount
train_data$interaction <- train_data$duration * train_data$total
test_data$interaction <- test_data$duration * test_data$total


#Example2: Adding ratio features
train_data$scan_to_total_ratio <- train_data$scans / train_data$total
test_data$scan_to_total_ratio <- test_data$scans / test_data$total

train_data$voided_to_scans_ratio <- train_data$voidedScans / train_data$scans
test_data$voided_to_scans_ratio <- test_data$voidedScans / test_data$scans

#Example3: Adding a composite risk score
train_data$riskScore <- train_data$voidedScans * 0.5 + train_data$attemptsWoScan * 0.5
test_data$riskScore <- test_data$voidedScans * 0.5 + test_data$attemptsWoScan * 0.5

```



#address data imbalance and gbm model (this is not the best model)
```{r}

library(ROSE)
library(DMwR2)
library(gbm)

train_data$fraud <- as.factor(train_data$fraud)

num_fraud <- nrow(train_data[train_data$fraud == "Fraud", ])
num_non_fraud <- nrow(train_data[train_data$fraud == "NonFraud", ])

additional_fraud_needed <- num_non_fraud - num_fraud
target_count <- num_fraud + additional_fraud_needed

data_upsampled <- ovun.sample(fraud ~ ., data = train_data, method = "over", N = target_count + num_non_fraud)$data
table(data_upsampled$fraud)

library(caret)
library(gbm)

# Prepare training control
train_control1 <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final",
  classProbs = TRUE,  # Save class probabilities for ROC analysis
  summaryFunction = twoClassSummary  # Use AUC for model tuning
)

# Set up a grid for hyperparameter tuning
gbmGrid <- expand.grid(
  interaction.depth = c(1, 3, 5),
  n.trees = c(100, 500, 1000),
  shrinkage = c(0.01, 0.05, 0.1),
  n.minobsinnode = c(10, 20)
)

set.seed(1111)
gbm_model1 <- train(
  fraud ~ .,
  data = data_upsampled,
  method = "gbm",
  trControl = train_control1,
  verbose = FALSE,
  metric = "ROC",
  tuneGrid = gbmGrid
)


print(gbm_model1)

importance <- summary(gbm_model1$finalModel, n.trees = 1000)
importance



library(gbm)
library(caret)

train_control2 <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final",
  classProbs = TRUE,  # Save class probabilities for ROC analysis
  summaryFunction = twoClassSummary  # Use AUC for model tuning
)

best_model_params2 <- expand.grid(
  interaction.depth = 5,     # Best depth
  n.trees = 1000,            # Best number of trees
  shrinkage = 0.1,           # Best shrinkage
  n.minobsinnode = 10        # Best minimum number of observations in node
)

gbm_best_model <- train(
  fraud ~ .,
  data = data_upsampled,
  method = "gbm",
  trControl = train_control2,
  verbose = FALSE,
  tuneGrid = best_model_params2,
  metric = "ROC"
)

print(gbm_best_model)



test_data$riskScore <- test_data$voidedScans * 0.5 + test_data$attemptsWoScan * 0.5

test_probabilities <- predict(gbm_best_model, newdata = test_data, type = "prob")
fraud_probabilities <- test_probabilities[, "Fraud"]

test_predictions <- ifelse(fraud_probabilities >= 0.3, 1, 0)

submission <- data.frame(id = test_data$id, fraud = test_predictions)
write.csv(submission, "GBM_kaggle_submission15.csv", row.names = FALSE)
```


#GBM model (without balacing data)

```{r}
library(caret)
library(gbm)

train_control <- trainControl(
  method = "cv",  # Cross-validation
  number = 10,    # Number of folds
  verboseIter = TRUE,  # Print training iterations
  savePredictions = "final",
  classProbs = TRUE,  # Save class probabilities for ROC analysis
  summaryFunction = twoClassSummary  # Use AUC for model tuning
)

gbmGrid <- expand.grid(
  interaction.depth = c(1, 3, 5),  # Depth of variable interactions
  n.trees = c(50, 100, 150,500,1000,1500),       # Number of trees
  shrinkage = c(0.01,0.00001, 0.1),        # Learning rate
  n.minobsinnode = c(10, 20)       # Minimum number of observations in the nodes
)

set.seed(1111)  # 
gbm_model <- train(
  fraud ~ .,  
  data = train_data,  
  method = "gbm",
  trControl = train_control,
  verbose = TRUE,
  metric = "ROC",
  tuneGrid = gbmGrid
)

#print(gbm_model)
#summary(gbm_model)

test_predictions <- predict(gbm_model, newdata = test_data, type = "prob")


test_predictions <- predict(gbm_model, newdata = test_data, type = "raw")


submission <- data.frame(id = test_data$id, fraud = as.numeric(test_predictions) - 1)
write.csv(submission, "GBM_Kaggle_Submission5.csv", row.names = FALSE)

```


#XGboost R&D



# Best Kaggle submission by xgboost  (this is best model)
```{r}
library(xgboost)
library(readr)
library(pROC)  

train_data <- read.csv("contest2data/train.csv")
test_data <- read.csv("contest2data/test.csv")

# Interaction between duration and total amount
train_data$interaction <- train_data$duration * train_data$total
test_data$interaction <- test_data$duration * test_data$total


train_features <- data.matrix(train_data[, !(names(train_data) %in% c("id", "fraud"))])
train_labels <- train_data$fraud
test_features <- data.matrix(test_data[, !(names(test_data) %in% c("id"))])

# Parameters
params <- list(booster = "gbtree", 
               objective = "binary:logistic",
               eta = 0.2, 
               max_depth = 6, 
               min_child_weight = 1,
               subsample = 1,
               colsample_bytree = 1)

dtrain <- xgb.DMatrix(data = train_features, 
                      label = train_labels)

xgb_model <- xgb.train(params = params, 
                       data = dtrain, 
                       nrounds = 500)

# Predict on training data
train_predictions <- predict(xgb_model, newdata = dtrain)

# Calculate AUC-ROC
roc_obj <- roc(train_labels, train_predictions)
auc_value <- auc(roc_obj)

# Plot ROC curve
plot(roc_obj, main = paste("AUC-ROC Curve (AUC =", round(auc_value, 2), ")"))

# Predict on test data
dtest <- xgb.DMatrix(data = test_features)
predictions_xgb <- predict(xgb_model, newdata = dtest)

# Create submission file
submission <- data.frame(id = test_data$id, 
                         fraud = as.numeric(predictions_xgb > 0.1))

write.csv(submission, "final_.csv", row.names = FALSE)

```








